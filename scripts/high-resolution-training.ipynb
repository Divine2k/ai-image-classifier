{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8497982,"sourceType":"datasetVersion","datasetId":5070657,"isSourceIdPinned":false},{"sourceId":709105,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":538573,"modelId":551843}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom PIL import Image, ImageFile\nimport torch\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import convnext_tiny\nimport os\nimport random\nimport shutil\nfrom torchvision import models\nimport kagglehub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:37:03.834061Z","iopub.execute_input":"2026-01-05T14:37:03.834851Z","iopub.status.idle":"2026-01-05T14:37:03.839564Z","shell.execute_reply.started":"2026-01-05T14:37:03.834824Z","shell.execute_reply":"2026-01-05T14:37:03.838755Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"Image.MAX_IMAGE_PIXELS = None\nImageFile.LOAD_TRUNCATED_IMAGES = True\ntorch.backends.cudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:37:06.867644Z","iopub.execute_input":"2026-01-05T14:37:06.868417Z","iopub.status.idle":"2026-01-05T14:37:06.874983Z","shell.execute_reply.started":"2026-01-05T14:37:06.868354Z","shell.execute_reply":"2026-01-05T14:37:06.874322Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\ntransform_val = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:52:07.294068Z","iopub.execute_input":"2026-01-05T14:52:07.294447Z","iopub.status.idle":"2026-01-05T14:52:07.300839Z","shell.execute_reply.started":"2026-01-05T14:52:07.294411Z","shell.execute_reply":"2026-01-05T14:52:07.300170Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"path = kagglehub.dataset_download(\"tristanzhang32/ai-generated-images-vs-real-images\")\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:57:04.236690Z","iopub.execute_input":"2026-01-05T12:57:04.236989Z","iopub.status.idle":"2026-01-05T12:57:04.421015Z","shell.execute_reply.started":"2026-01-05T12:57:04.236962Z","shell.execute_reply":"2026-01-05T12:57:04.420262Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/ai-generated-images-vs-real-images\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!echo \"Train AI:\" && ls /kaggle/working/subset_dataset/val/fake | wc -l\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:40:26.592609Z","iopub.execute_input":"2026-01-05T14:40:26.592969Z","iopub.status.idle":"2026-01-05T14:40:26.755041Z","shell.execute_reply.started":"2026-01-05T14:40:26.592935Z","shell.execute_reply":"2026-01-05T14:40:26.754305Z"}},"outputs":[{"name":"stdout","text":"Train AI:\n2955\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"train_dataset = datasets.ImageFolder(root='/kaggle/working/subset_dataset/train', transform=transform_train)\nvalidation_dataset = datasets.ImageFolder(root='/kaggle/working/subset_dataset/val', transform=transform_val)\ntest_dataset = datasets.ImageFolder(root='/kaggle/working/subset_dataset/test', transform=transform_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:52:26.392299Z","iopub.execute_input":"2026-01-05T14:52:26.393088Z","iopub.status.idle":"2026-01-05T14:52:26.455417Z","shell.execute_reply.started":"2026-01-05T14:52:26.393060Z","shell.execute_reply":"2026-01-05T14:52:26.454801Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"test_dataset.class_to_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:52:44.495783Z","iopub.execute_input":"2026-01-05T14:52:44.496083Z","iopub.status.idle":"2026-01-05T14:52:44.501247Z","shell.execute_reply.started":"2026-01-05T14:52:44.496056Z","shell.execute_reply":"2026-01-05T14:52:44.500716Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"{'fake': 0, 'real': 1}"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\nvalidation_loader = DataLoader(validation_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:53:39.721556Z","iopub.execute_input":"2026-01-05T14:53:39.721879Z","iopub.status.idle":"2026-01-05T14:53:39.733949Z","shell.execute_reply.started":"2026-01-05T14:53:39.721854Z","shell.execute_reply":"2026-01-05T14:53:39.733188Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"images, labels = next(iter(train_loader))\nprint(images.shape)\nprint(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:53:44.676324Z","iopub.execute_input":"2026-01-05T14:53:44.677103Z","iopub.status.idle":"2026-01-05T14:54:19.639159Z","shell.execute_reply.started":"2026-01-05T14:53:44.677075Z","shell.execute_reply":"2026-01-05T14:54:19.638140Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 3, 224, 224])\ntensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 0, 0])\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"device=''\nif torch.cuda.is_available():\n    device='cuda'\nelse:\n    device='cpu'\n\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:54:32.306728Z","iopub.execute_input":"2026-01-05T14:54:32.307593Z","iopub.status.idle":"2026-01-05T14:54:32.311950Z","shell.execute_reply.started":"2026-01-05T14:54:32.307556Z","shell.execute_reply":"2026-01-05T14:54:32.311325Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"model = convnext_tiny(weights=None)\nin_features = model.classifier[2].in_features\nmodel.classifier[2] = nn.Linear(in_features, 1)\nmodel = model.to(device, memory_format=torch.channels_last)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:54:37.478779Z","iopub.execute_input":"2026-01-05T14:54:37.479356Z","iopub.status.idle":"2026-01-05T14:54:37.999126Z","shell.execute_reply.started":"2026-01-05T14:54:37.479325Z","shell.execute_reply":"2026-01-05T14:54:37.998189Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"ckpt_path = '/kaggle/input/convnext/pytorch/default/1/convNext_final.pth'\nmodel.load_state_dict(torch.load(ckpt_path, map_location=device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:54:43.158707Z","iopub.execute_input":"2026-01-05T14:54:43.159210Z","iopub.status.idle":"2026-01-05T14:54:44.335014Z","shell.execute_reply.started":"2026-01-05T14:54:43.159179Z","shell.execute_reply":"2026-01-05T14:54:44.334425Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\n\nfor param in model.classifier[2].parameters():\n    param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:54:51.047775Z","iopub.execute_input":"2026-01-05T14:54:51.048433Z","iopub.status.idle":"2026-01-05T14:54:51.052801Z","shell.execute_reply.started":"2026-01-05T14:54:51.048405Z","shell.execute_reply":"2026-01-05T14:54:51.052147Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.classifier[2].parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:54:52.626091Z","iopub.execute_input":"2026-01-05T14:54:52.626730Z","iopub.status.idle":"2026-01-05T14:54:52.630675Z","shell.execute_reply.started":"2026-01-05T14:54:52.626702Z","shell.execute_reply":"2026-01-05T14:54:52.630029Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"scaler = torch.amp.GradScaler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:55:01.701707Z","iopub.execute_input":"2026-01-05T14:55:01.702211Z","iopub.status.idle":"2026-01-05T14:55:01.705622Z","shell.execute_reply.started":"2026-01-05T14:55:01.702183Z","shell.execute_reply":"2026-01-05T14:55:01.704933Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"def train(model, loader, optimizer, criterion):\n\n    model.train()\n    train_loss = 0.0\n\n    for i, (inputs, labels) in enumerate(loader):\n\n        inputs = inputs.to(device)\n        labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n\n        with torch.amp.autocast(device_type=device):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item() * inputs.size(0)\n\n    return train_loss / len(loader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:55:21.110550Z","iopub.execute_input":"2026-01-05T14:55:21.111133Z","iopub.status.idle":"2026-01-05T14:55:21.116202Z","shell.execute_reply.started":"2026-01-05T14:55:21.111105Z","shell.execute_reply":"2026-01-05T14:55:21.115641Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"def validate(model, loader, criterion):\n\n    model.eval()\n    running_loss  = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n\n        for i, (inputs, labels) in enumerate(loader):\n\n            inputs = inputs.to(device)\n            labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n\n            probs = torch.sigmoid(outputs)\n            preds = (probs > 0.5).float()\n\n            correct += (preds.squeeze(1) == labels.squeeze(1)).sum().item()\n            total += labels.size(0)\n            running_loss += loss.item() * inputs.size(0)\n\n\n    avg_loss = running_loss / total\n    accuracy = correct / total\n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:55:21.867905Z","iopub.execute_input":"2026-01-05T14:55:21.868501Z","iopub.status.idle":"2026-01-05T14:55:21.874760Z","shell.execute_reply.started":"2026-01-05T14:55:21.868459Z","shell.execute_reply":"2026-01-05T14:55:21.873955Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"epochs = 5\nPATIENCE = 2\nbest_val_acc = 0.0\npatience_counter = 0\n\nfor epoch in range(epochs):\n\n    train_loss = train(model, train_loader, optimizer, criterion)\n    val_loss, val_acc = validate(model, validation_loader, criterion)\n\n    print(f\"\"\"Epoch [{epoch+1}/{epochs}]\n            Train Loss: {train_loss:.4f}\n            Val Loss:   {val_loss:.4f}\n            Val Acc:    {val_acc:.4f}\n    \"\"\")\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if patience_counter >= PATIENCE:\n        print(\"Early stopping triggered\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:55:41.760108Z","iopub.execute_input":"2026-01-05T14:55:41.760434Z","iopub.status.idle":"2026-01-05T15:51:59.746023Z","shell.execute_reply.started":"2026-01-05T14:55:41.760405Z","shell.execute_reply":"2026-01-05T15:51:59.745215Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5]\n            Train Loss: 2.4002\n            Val Loss:   2.3269\n            Val Acc:    0.5920\n    \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/5]\n            Train Loss: 2.1461\n            Val Loss:   2.0765\n            Val Acc:    0.6072\n    \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/5]\n            Train Loss: 1.9135\n            Val Loss:   1.8440\n            Val Acc:    0.6226\n    \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/5]\n            Train Loss: 1.6785\n            Val Loss:   1.6288\n            Val Acc:    0.6347\n    \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/5]\n            Train Loss: 1.4895\n            Val Loss:   1.4299\n            Val Acc:    0.6476\n    \n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the paths to remove\nsubset_folder = '/kaggle/working/subset_dataset'\nzip_file = '/kaggle/working/subset_dataset.zip'\n\n# Remove the unzipped folder\nif os.path.exists(subset_folder):\n    shutil.rmtree(subset_folder)\n    print(f\"Deleted folder: {subset_folder}\")\n\n# Remove the zip file (only do this after you have downloaded it!)\nif os.path.exists(zip_file):\n    os.remove(zip_file)\n    print(f\"Deleted zip: {zip_file}\")\n\n# Optional: Clear everything in working directory (be careful!)\n# for filename in os.listdir('/kaggle/working'):\n#     file_path = os.path.join('/kaggle/working', filename)\n#     if os.path.isfile(file_path) or os.path.islink(file_path):\n#         os.unlink(file_path)\n#     elif os.path.isdir(file_path):\n#         shutil.rmtree(file_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T14:28:16.187087Z","iopub.execute_input":"2026-01-05T14:28:16.187871Z","iopub.status.idle":"2026-01-05T14:28:16.456074Z","shell.execute_reply.started":"2026-01-05T14:28:16.187843Z","shell.execute_reply":"2026-01-05T14:28:16.455355Z"}},"outputs":[{"name":"stdout","text":"Deleted folder: /kaggle/working/subset_dataset\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"for p in model.parameters():\n    p.requires_grad = False\nfor p in model.features[-1].parameters():\n    p.requires_grad = True\nfor p in model.classifier[2].parameters():\n    p.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T15:59:43.771538Z","iopub.execute_input":"2026-01-05T15:59:43.771998Z","iopub.status.idle":"2026-01-05T15:59:43.780101Z","shell.execute_reply.started":"2026-01-05T15:59:43.771964Z","shell.execute_reply":"2026-01-05T15:59:43.779328Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(\n    [\n        {\"params\": model.features[-1].parameters(), \"lr\": 1e-5},\n        {\"params\": model.classifier[2].parameters(),     \"lr\": 1e-4},\n    ],\n    weight_decay=1e-4\n)\n\ncriterion = nn.BCEWithLogitsLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T16:02:49.056503Z","iopub.execute_input":"2026-01-05T16:02:49.057345Z","iopub.status.idle":"2026-01-05T16:02:49.065494Z","shell.execute_reply.started":"2026-01-05T16:02:49.057313Z","shell.execute_reply":"2026-01-05T16:02:49.064769Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"EPOCHS = 14\nPATIENCE = 3\nbest_val_acc = 0.0\npatience_counter = 0\n\n\nfor epoch in range(EPOCHS):\n    train_loss = train(model, train_loader, optimizer, criterion)\n    val_loss, val_acc = validate(model, validation_loader, criterion)\n\n    print(\n        f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n        f\"Train Loss: {train_loss:.4f} | \"\n        f\"Val Loss: {val_loss:.4f} | \"\n        f\"Val Acc: {val_acc:.4f}\"\n    )\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    if patience_counter >= PATIENCE:\n        print(\"Early stopping triggered\")\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loss, test_acc = validate(\n    model,\n    test_loader,\n    criterion\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:43:38.222473Z","iopub.execute_input":"2026-01-05T18:43:38.223311Z","iopub.status.idle":"2026-01-05T18:46:49.901829Z","shell.execute_reply.started":"2026-01-05T18:43:38.223259Z","shell.execute_reply":"2026-01-05T18:46:49.900974Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"print(f\"TEST Loss: {test_loss:.4f}\")\nprint(f\"TEST Accuracy: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:47:12.503705Z","iopub.execute_input":"2026-01-05T18:47:12.504040Z","iopub.status.idle":"2026-01-05T18:47:12.508666Z","shell.execute_reply.started":"2026-01-05T18:47:12.504006Z","shell.execute_reply":"2026-01-05T18:47:12.508012Z"}},"outputs":[{"name":"stdout","text":"TEST Loss: 0.2070\nTEST Accuracy: 0.9158\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the paths to remove\nsubset_folder = '/kaggle/working/subset_dataset'\nzip_file = '/kaggle/working/subset_dataset.zip'\n\n# Remove the unzipped folder\nif os.path.exists(subset_folder):\n    shutil.rmtree(subset_folder)\n    print(f\"Deleted folder: {subset_folder}\")\n\n# Remove the zip file (only do this after you have downloaded it!)\nif os.path.exists(zip_file):\n    os.remove(zip_file)\n    print(f\"Deleted zip: {zip_file}\")\n\n# Optional: Clear everything in working directory (be careful!)\n# for filename in os.listdir('/kaggle/working'):\n#     file_path = os.path.join('/kaggle/working', filename)\n#     if os.path.isfile(file_path) or os.path.islink(file_path):\n#         os.unlink(file_path)\n#     elif os.path.isdir(file_path):\n#         shutil.rmtree(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:50:00.585580Z","iopub.execute_input":"2026-01-05T18:50:00.585992Z","iopub.status.idle":"2026-01-05T18:50:03.347469Z","shell.execute_reply.started":"2026-01-05T18:50:00.585961Z","shell.execute_reply":"2026-01-05T18:50:03.345791Z"}},"outputs":[{"name":"stdout","text":"Deleted folder: /kaggle/working/subset_dataset\nDeleted zip: /kaggle/working/subset_dataset.zip\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"torch.save(\n    model.state_dict(),\n    \"/kaggle/working/convnext_tiny_final.pth\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T18:51:18.389289Z","iopub.execute_input":"2026-01-05T18:51:18.390218Z","iopub.status.idle":"2026-01-05T18:51:18.546889Z","shell.execute_reply.started":"2026-01-05T18:51:18.390182Z","shell.execute_reply":"2026-01-05T18:51:18.546030Z"}},"outputs":[],"execution_count":84}]}